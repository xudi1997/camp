{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7efe55102860>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-datasets/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7efe55102f98>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-datasets/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7efe55102e48>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-datasets/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7efe55102630>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-datasets/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7efe55102550>: Failed to establish a new connection: [Errno 101] Network is unreachable',)': /simple/tensorflow-datasets/\u001b[0m\n",
      "\u001b[31m  Could not find a version that satisfies the requirement tensorflow-datasets (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for tensorflow-datasets\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7bd450b0f03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#import tensorflow_datasets as tfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmax_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deecamp/cifar10.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Process images of this size. Note that this differs from the original CIFAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import math\n",
    "import cifar10\n",
    "import numpy as np\n",
    "max_step=3000\n",
    "batch_size=128\n",
    "num_examples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_images_labels(batch_size, split, distords=False):\n",
    "#     \"\"\"Returns Dataset for given split.\"\"\"\n",
    "#     dataset = tfds.load(name='cifar10', split=split)\n",
    "#     scope = 'data_augmentation' if distords else 'input'\n",
    "#     with tf.name_scope(scope):\n",
    "#         dataset = dataset.map(DataPreprocessor(distords), num_parallel_calls=10)\n",
    "#     # Dataset is small enough to be fully loaded on memory:\n",
    "#     dataset = dataset.prefetch(-1)\n",
    "#     dataset = dataset.repeat().batch(batch_size)\n",
    "#     iterator = dataset.make_one_shot_iterator()\n",
    "#     images_labels = iterator.get_next()\n",
    "#     images, labels = images_labels['input'], images_labels['target']\n",
    "#     tf.summary.image('images', images)\n",
    "#     return images, labels\n",
    "\n",
    "# def distorted_inputs(batch_size):\n",
    "#     return _get_images_labels(batch_size, tfds.Split.TRAIN, distords=True)\n",
    "\n",
    "# def inputs(eval_data, batch_size):\n",
    "#     split = tfds.Split.TEST if eval_data == 'test' else tfds.Split.TRAIN\n",
    "#     return _get_images_labels(batch_size, split)\n",
    "\n",
    "def variable_with_weight_loss(shape,stddev,w):\n",
    "    var = tf.Variable(tf.truncated_normal(shape,stddev=stddev))\n",
    "    if w is not None:\n",
    "        weight_loss = tf.multiply(tf.nn.l2_loss(var),w,name='weight_loss')\n",
    "        tf.add_to_collection('losses',weight_loss)\n",
    "    return var\n",
    "\n",
    "def loss(logits,labels):\n",
    "    labels = tf.cast(labels,tf.int64)\n",
    "    cross_entropy = tf.nn.sparse.softmax_cross_entropy_with_logits(logits=logits,labels=labels,name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy,name='cross_entropy')\n",
    "    tf.add_to_collection('losses',cross_entropy_mean)\n",
    "    return tf.add_n(tf.get_collection('losses'),name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net define\n",
    "images_train,labels_train = cifar10.distorted_inputs(batch_size)\n",
    "images_test,labels_test = cifar10.inputs(eval_data=True,batch_size=batch_size)\n",
    "\n",
    "image_holder = tf.placeholder(tf.float32,shape=[batch_size,24,24,3])\n",
    "label_holder = tf.placeholder(tf.float32,shape=[batch_size])\n",
    "\n",
    "w1 = variable_with_weight_loss(shape=[5,5,3,64],stddev=5e-2,w=0.0)\n",
    "b1 = tf.Variable(tf.constant(0.0,shape=[64]))\n",
    "h1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(image_holder,w1,[1,1,1,1],padding='SAME'),b1))\n",
    "h1_pool = tf.nn.max_pool(h1,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')\n",
    "norml = tf.nn.lrn(h1_pool,4,bias=1.0,padding=0.001/9.0,beta=0.75)\n",
    "\n",
    "w2 = variable_with_weight_loss(shape=[5,5,64,64],stddev=5e-2,w=0.0)\n",
    "b2 = tf.Variable(tf.constant(0.1,shape=[64]))\n",
    "h2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(norml,w2,[1,1,1,1],padding='SAME'),b2))\n",
    "norm2 = tf.nn.lrn(h2,4,bias=1.0,padding=0.001/9.0,beta=0.75)\n",
    "h2_pool = tf.nn.max_pool(norm2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')\n",
    "\n",
    "reshape = tf.reshape(h2_pool,[batch_size,-1])\n",
    "dim = reshape.get_shape()[1].value\n",
    "w3 = variable_with_weight_loss(shape=[dim,384],stddev=0.04,w=0.004)\n",
    "b3 = tf.Variable(tf.constant(0.1,shape=[384]))\n",
    "fc3 = tf.nn.relu(tf.matmul(reshape,w3)+b3)\n",
    "\n",
    "w4 = variable_with_weight_loss(shape=[384,192],stddev=0.04,w=0.004)\n",
    "b4 = tf.Variable(tf.constant(0.1,shape=[192]))\n",
    "fc4 = tf.nn.relu(tf.matmul(fc3,w4)+b4)\n",
    "\n",
    "w5 = variable_with_weight_loss(shape=[192,10],stddev=1/192.0,w=0.0)\n",
    "b5 = tf.Variable(tf.constant(0.1,shape=[10]))\n",
    "logits = tf.add(tf.matmul(fc4,w5),b5)\n",
    "\n",
    "loss = loss(logits,label_holder)\n",
    "train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.start_queue_runners()\n",
    "    for step in range(max_steps):\n",
    "        image_batch,label_batch = sess.run([images_train,labels_train])\n",
    "        _,loss_value = sess.run([train_op,loss],{image_holder:image_batch,label_holder:label_batch})\n",
    "        if step % 10==0:\n",
    "            print('step %d,loss value %f'%(step,loss_value))\n",
    "    saver.save(sess,'cifar.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ed1b01fb456e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrue_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_sample_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "num_iter = int(math.ceil(num_examples/batch_size))\n",
    "true_count=0\n",
    "total_sample_count = num_iter*batch_size\n",
    "step=0\n",
    "while step<num_iter:\n",
    "    image_batch,label_batch = sess.run([images_test,labels_test])\n",
    "    predictions = sess.run([top_k_op],{image_holder:image_batch,label_holder:label_batch})\n",
    "    true_count +=np.sum(predictions)\n",
    "    step+=1\n",
    "precision = true_count/total_sample_count\n",
    "print('precision:',\"{:.3f}\".format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
